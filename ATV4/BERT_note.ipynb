{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = pd.read_csv(\"Dmoz-sports.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets for model training\n",
    "train_dataset = BertDataset(train_encodings, train_labels)\n",
    "val_dataset = BertDataset(val_encodings, val_labels)\n",
    "test_dataset = BertDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "num_labels = len(label_encoder.classes_)\n",
    "model = get_model(num_labels)\n",
    "\n",
    "# Train model\n",
    "trainer = train_model(model, train_dataset, val_dataset, tokenizer)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy, f1_micro, f1_macro, conf_matrix = evaluate_model(trainer, test_dataset, test_labels, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "# Tokenize the text data for each split\n",
    "# Tokenize training data and get analysis samples\n",
    "train_encodings, train_sample_text, train_tokens, train_token_ids = tokenize_texts(train_texts, tokenizer)\n",
    "\n",
    "# Tokenize validation and test data (only need encodings)\n",
    "val_encodings = tokenize_texts(val_texts, tokenizer)[0]\n",
    "test_encodings = tokenize_texts(test_texts, tokenizer)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
